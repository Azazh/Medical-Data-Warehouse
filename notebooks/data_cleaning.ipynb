{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import logging\n",
    "from datetime import datetime\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler('data_cleaning.log'),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "\n",
    "class DataCleaner:\n",
    "    def __init__(self, raw_data_dir='../scripts/raw_data/archive/archive', db_connection='postgresql://postgres:5492460@localhost:5432/medical_dw'):\n",
    "        \"\"\"\n",
    "        Initialize the DataCleaner with raw data directory and database connection.\n",
    "        \"\"\"\n",
    "        self.raw_data_dir = raw_data_dir\n",
    "        self.engine = create_engine(db_connection)\n",
    "        self.processed_files = set()\n",
    "\n",
    "    def load_raw_data(self):\n",
    "        \"\"\"\n",
    "        Load and merge all JSON files from the raw_data directory.\n",
    "        \"\"\"\n",
    "        all_data = []\n",
    "        for filename in os.listdir(self.raw_data_dir):\n",
    "            if filename.endswith('.json'):\n",
    "                filepath = os.path.join(self.raw_data_dir, filename)\n",
    "                with open(filepath, 'r', encoding='utf-8') as f:\n",
    "                    try:\n",
    "                        data = json.load(f)\n",
    "                        all_data.extend(data)\n",
    "                        self.processed_files.add(filename)\n",
    "                        logging.info(f\"Loaded {len(data)} records from {filename}\")\n",
    "                    except json.JSONDecodeError:\n",
    "                        logging.error(f\"Invalid JSON format in {filename}\")\n",
    "                    except Exception as e:\n",
    "                        logging.error(f\"Error loading {filename}: {str(e)}\")\n",
    "        return pd.DataFrame(all_data)\n",
    "\n",
    "    def clean_data(self, df):\n",
    "        \"\"\"\n",
    "        Perform data cleaning transformations.\n",
    "        \"\"\"\n",
    "        # Remove duplicates\n",
    "        df = df.drop_duplicates(subset=['message_id', 'channel'], keep='last')\n",
    "        logging.info(f\"Removed duplicates, remaining records: {len(df)}\")\n",
    "\n",
    "        # Handle missing values\n",
    "        df['text'] = df['text'].fillna('')\n",
    "        df['media_path'] = df['media_path'].fillna('N/A')\n",
    "        logging.info(\"Handled missing values\")\n",
    "\n",
    "        # Standardize formats\n",
    "        df['date'] = pd.to_datetime(df['date'], errors='coerce')\n",
    "        df['scrape_timestamp'] = pd.to_datetime(df['scrape_timestamp'])\n",
    "        logging.info(\"Standardized date formats\")\n",
    "\n",
    "        # Clean text data\n",
    "        df['text_clean'] = df['text'].str.strip().str.lower()\n",
    "        logging.info(\"Cleaned text data\")\n",
    "\n",
    "        # Data validation\n",
    "        valid_mask = df['date'].notna() & df['text_clean'].ne('')\n",
    "        df = df[valid_mask].copy()\n",
    "        logging.info(f\"Applied validation, remaining records: {len(df)}\")\n",
    "        return df\n",
    "\n",
    "    def save_to_db(self, clean_df):\n",
    "        \"\"\"\n",
    "        Save cleaned data to PostgreSQL database.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            clean_df.to_sql(\n",
    "                'raw_medical_data',\n",
    "                self.engine,\n",
    "                if_exists='append',\n",
    "                index=False,\n",
    "                method='multi'\n",
    "            )\n",
    "            logging.info(f\"Successfully loaded {len(clean_df)} records to database\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Database insertion failed: {str(e)}\")\n",
    "\n",
    "    def archive_raw_files(self):\n",
    "        \"\"\"\n",
    "        Move processed files to archive directory.\n",
    "        \"\"\"\n",
    "        archive_dir = os.path.join(self.raw_data_dir, 'archive')\n",
    "        os.makedirs(archive_dir, exist_ok=True)\n",
    "        \n",
    "        for filename in self.processed_files:\n",
    "            src = os.path.join(self.raw_data_dir, filename)\n",
    "            dest = os.path.join(archive_dir, filename)\n",
    "            os.rename(src, dest)\n",
    "            logging.info(f\"Archived {filename}\")\n",
    "\n",
    "    def run_pipeline(self):\n",
    "        \"\"\"\n",
    "        Execute the full data cleaning pipeline.\n",
    "        \"\"\"\n",
    "        logging.info(\"Starting data cleaning pipeline\")\n",
    "        \n",
    "        raw_df = self.load_raw_data()\n",
    "        logging.info(f\"Loaded {len(raw_df)} raw records\")\n",
    "        \n",
    "        clean_df = self.clean_data(raw_df)\n",
    "        logging.info(f\"Cleaned data contains {len(clean_df)} records\")\n",
    "        \n",
    "        self.save_to_db(clean_df)\n",
    "        self.archive_raw_files()\n",
    "        logging.info(\"Data cleaning pipeline completed successfully\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    cleaner = DataCleaner()\n",
    "    cleaner.run_pipeline()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_center_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
